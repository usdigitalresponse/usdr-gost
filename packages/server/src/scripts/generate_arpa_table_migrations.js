require('dotenv').config();

// NOTE(mbroussard): need to run with ARPA_POSTGRES_URL set manually; POSTGRES_URL will be GOST URL
const { ARPA_POSTGRES_URL } = process.env;
if (!ARPA_POSTGRES_URL) {
    throw new Error('run with ARPA_POSTGRES_URL env var');
}

const { promisify } = require('util');
const { exec: origExec } = require('child_process');

const exec = promisify(origExec);
const fs = require('fs/promises');
const path = require('path');
const knex = require('../db/connection');

// NOTE(mbroussard): this ordering is sensitive since some of these tables have FK constraints on the others
const TABLES = [
    // TODO(mbroussard): do we want to rename any of these ARPA tables when moving into GOST?
    'reporting_periods',
    'uploads', // has FK to reporting_periods
    'arpa_subrecipients', // has FK to uploads
    'application_settings', // has FK to reporting_periods
    'projects', // has FK to reporting_periods
    'period_summaries', // has FK to reporting_periods, projects
];

function pgDumpCommandTemplate(url, tableName) {
    return `pg_dump ${url} --schema-only --no-owner --no-privileges --table=${tableName}`;
}

function multiplyString(str, n) {
    let ret = '';
    for (let i = 0; i < n; i += 1) {
        ret += str;
    }
    return ret;
}

const fourSpaces = '    ';
function indent(str, level, delimiter = fourSpaces, skipFirstLine = true) {
    const indentation = multiplyString(delimiter, level);
    return str
        .split('\n')
        .map((line, i) => (i > 0 || !skipFirstLine ? indentation + line : line))
        .join('\n');
}

function migrationTemplate({
    pgDumpOutput, pgDumpCommand, tableName, runDate,
}) {
    // This line outputted by pg_dump causes problems in Knex migrations, see
    // https://stackoverflow.com/a/70963201
    const searchPathConfig = 'SELECT pg_catalog.set_config(\'search_path\', \'\', false);';
    pgDumpOutput = pgDumpOutput.replace(
        searchPathConfig,
        `-- Line below commented out by generate_arpa_table_migrations.js because it interferes with Knex\n-- ${searchPathConfig}`,
    );

    const text = `
/* eslint-disable func-names */

// This file was generated by generate_arpa_table_migrations.js on ${runDate.toISOString()}.
// Describe any manual modifications below:
//  - (none)

exports.up = function (knex) {
    return knex.schema.raw(
        // This SQL generated with the following command:
        // ${pgDumpCommand}
        \`
            ${indent(pgDumpOutput, 3)}
        \`,
    );
};

exports.down = function (knex) {
    return knex.schema.dropTable('${tableName}');
};
    `;
    return `${text.trim()}\n`;
}

async function main() {
    const runDate = new Date();
    /* eslint-disable no-await-in-loop */
    for (let i = 0; i < TABLES.length; i += 1) {
        const tableName = TABLES[i];
        const idx = String(i + 1).padStart(2, '0');

        // First run pg_dump to get the table DDL
        const pgDumpCommand = pgDumpCommandTemplate(ARPA_POSTGRES_URL, tableName);
        let { stdout: pgDumpOutput } = await exec(pgDumpCommand);
        pgDumpOutput = pgDumpOutput.trim();

        // Then create a new Knex migration.
        // We include an extra incrementing number in the filename to ensure these run in the
        // order we create them since the seconds-resolution filename timestamps will all be
        // the same.
        const migrationName = `arpa_integration_create_table_${idx}_${tableName}`;
        const migrationPath = await knex.migrate.make(migrationName);
        const migrationFilename = path.basename(migrationPath);

        // Finally, write the dumped DDL into the generated migration file
        const generatedMigration = migrationTemplate({
            pgDumpOutput, pgDumpCommand, tableName, runDate,
        });
        await fs.writeFile(migrationPath, generatedMigration, { flag: 'w' });

        console.log('Created migration', migrationFilename);
    }
}

if (require.main === module) {
    main();
}
